{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linkpan1/tflite-hw/blob/main/tflite_training_sinewave.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHZu8_baedPI"
      },
      "source": [
        "# Make sure you are using the right TensorFlow Version\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1MIqCPXjE0P"
      },
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1XIqaQuiWrD"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "# Keras is TensorFlow's high-level API for deep learning\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-aHKkNCjj23"
      },
      "source": [
        "# Generate data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RHqwvS0jBzk"
      },
      "source": [
        "# Number of sample datapoints\n",
        "SAMPLES = 1500\n",
        "\n",
        "# Set a \"seed\" value, so we get the same random numbers each time we run this\n",
        "# notebook for reproducible results. Any number can be used here.\n",
        "np.random.seed(786)\n",
        "tf.random.set_seed(786)\n",
        "\n",
        "# Generate a uniformly distributed set of random numbers in the range from\n",
        "# 0 to 2Ï€, which covers a complete sine wave oscillation\n",
        "x_values = np.random.uniform(low=0, high=2*math.pi, size=SAMPLES)\n",
        "\n",
        "# Let's visualize x_values to see if the x_values are random or\n",
        "# concentrated in one area\n",
        "plt.plot(x_values)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pO7kPqxktEK"
      },
      "source": [
        "# You can also shuffle the values to guarantee they're not in order\n",
        "np.random.shuffle(x_values)\n",
        "\n",
        "# Calculate the corresponding sine values\n",
        "y_values = np.sin(x_values)\n",
        "\n",
        "# Plot our data\n",
        "plt.plot(x_values, y_values, 'b.')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqYedzpGlQuK"
      },
      "source": [
        "# Add noise\n",
        "Real world sensor data is rarely this smooth. For example, if you would have collect an accelerometer data for training it would have been noisy. To reflect real-world situation let's add some noise to the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vtp1x0b-lntM"
      },
      "source": [
        "# Add a small random number to each y value\n",
        "y_values += 0.1 * np.random.randn(*y_values.shape)\n",
        "\n",
        "# Plot our data\n",
        "plt.plot(x_values, y_values, 'b.')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1zmZVDCx1xQ"
      },
      "source": [
        "We now have a noisy dataset that approximates real world data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWCmUlZwnaxV"
      },
      "source": [
        "# Split the Data\n",
        "It is a common practice to split the data into 60-20-20 for training and testing. The data is split as follows:\n",
        "  1. Training: 60%\n",
        "  2. Validation: 20%\n",
        "  3. Testing: 20%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndLEPdD3n6Nm"
      },
      "source": [
        "# We'll use 60% of our data for training and 20% for validation. The remaining 20%\n",
        "# will be used for testing. Calculate the indices of each section.\n",
        "TRAIN_SPLIT =  int(0.6 * SAMPLES)\n",
        "TEST_SPLIT = int(0.2 * SAMPLES + TRAIN_SPLIT)\n",
        "\n",
        "# Use np.split to chop our data into three parts.\n",
        "# The second argument to np.split is an array of indices where the data will be\n",
        "# split. We provide two indices, so the data will be divided into three chunks.\n",
        "x_train, x_test, x_validate = np.split(x_values, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "y_train, y_test, y_validate = np.split(y_values, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "\n",
        "# Double check that our splits add up correctly\n",
        "assert (x_train.size + x_validate.size + x_test.size) ==  SAMPLES\n",
        "\n",
        "# Plot the data in each partition.\n",
        "# We have to make sure that each set, train, validation and test, has the full\n",
        "# range of x values, 0 to 2pi\n",
        "plt.plot(x_train, y_train, 'b.', label=\"Train\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(x_validate, y_validate, 'y.', label=\"Validate\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(x_test, y_test, 'r.', label=\"Test\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPvAiZKxpq--"
      },
      "source": [
        "# Design the model\n",
        "The model is going to predict a numeric value based on a numeric input. This type of problem is known as regression. We are going to solve this using neural network.\n",
        "\n",
        "In neural network you have neurons (think of it as a node in a [mesh network](https://en.wikipedia.org/wiki/Mesh_networking)). Each of these neurons has weight and bias value. During training, these values are changed, by a _**activation function**_, that you will choose, to match its prediction with the actual output. A _**loss function**_ will be used to see how far the predictions are from the actual value and the training process will try to minimize this value.\n",
        "\n",
        "You can have any numbers of layers of neurons. But the more neurons leads to more complexity, and hence will also increase the size of the model. We will be using two layers of 16 neurons (i.e. 32 neurons), with one input layer and one output layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McqBIP4KpuH6"
      },
      "source": [
        "# We'll use Keras to create a simple model architecture\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "# First layer takes a scalar input and feeds it through 16 \"neurons\". The\n",
        "# neurons decide whether to activate based on the 'relu' activation function.\n",
        "model.add(keras.layers.Dense(16, activation='relu', input_shape=(1,)))\n",
        "\n",
        "# The second layer of 16 neurons. Note input connected to the first layer.\n",
        "# As this is a sequential model, all the first layer neuron will be connected\n",
        "# to this second layer.\n",
        "model.add(keras.layers.Dense(16, activation='relu'))\n",
        "\n",
        "# Final layer is a single neuron, since we want to output a single value\n",
        "model.add(keras.layers.Dense(1))\n",
        "\n",
        "# Compile the model using a standard optimizer and loss function for regression\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22LA1NHiLK-D"
      },
      "source": [
        "# Train the model\n",
        "During training, model will predict the output of a corrsponding input `x` and will check how far it is from the actual value. then it will adjust the neurons' weights and biases to match the actual output.\n",
        "\n",
        "Epoch: Training runs this process on the full dataset multiple times, and each full run-through is known as an epoch and we can set this parameter. Don't use high number of epochs. Otherwise the model will overfit their training data.\n",
        "\n",
        "Batch Size: During each epoch, you can adjust the weights and biases after each input. Or you can update those values in batches. For example, use 16 samples, aggregate their correctness results and then update weights and biases based on that. Choosing 1 as batch size will take forever to train, choosing the whole data as batch size will result in less accurate model. Its a trial and error situation. Thumb of rule is start with a batch size of 16 or 32 and increase from their to see what works best for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owYDOejkLP5P"
      },
      "source": [
        "training_info = model.fit(x_train, y_train, epochs=350, batch_size=64, validation_data=(x_validate, y_validate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSSzWMNwOaR_"
      },
      "source": [
        "# Plot Metrics\n",
        "During each training epoch, the model prints out its loss and mean absolute error for training and validation as you can see in the output above (note that your exact numbers may differ):\n",
        "\n",
        "```\n",
        "Epoch 350/350\n",
        "15/15 [==============================] - 0s 4ms/sample - loss: 0.0105 - mae: 0.0809 - val_loss: 0.0108 - val_mae: 0.0803\n",
        "```\n",
        "\n",
        "Let's see that on a graph how model's performance changed over time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld65hJ_yPDLi"
      },
      "source": [
        "# Draw a graph of the loss, which is the distance between\n",
        "# the predicted and actual values during training and validation.\n",
        "loss = training_info.history['loss']\n",
        "validation_loss = training_info.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.plot(epochs, loss, 'g.', label='Training loss')\n",
        "plt.plot(epochs, validation_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHkfuCTjREVK"
      },
      "source": [
        "The loss rapidly decresed at the beginnig before flattening out at the end. To make flatter part more readable let's skip first 50 epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEUU3UxRRi6i"
      },
      "source": [
        "# Exclude the first few epochs so the graph is easier to read\n",
        "SKIP = 50\n",
        "\n",
        "plt.plot(epochs[SKIP:], loss[SKIP:], 'g.', label='Training loss')\n",
        "plt.plot(epochs[SKIP:], validation_loss[SKIP:], 'b.', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVoQ_aQ2RsiL"
      },
      "source": [
        "From the plot, we can see that loss continues to reduce until around 200 epochs, at which point it is mostly stable.\n",
        "\n",
        "We can also see that the lowest loss value is around 0.0108. This means that our network's predictions are off by an average of ~1%. Which is really good.\n",
        "\n",
        "Let's plot the mean absolute error, which is another way of measuring how far the network's predictions are from the actual numbers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtUDXPtuR9ER"
      },
      "source": [
        "plt.clf()\n",
        "\n",
        "# Draw a graph of mean absolute error, which is another way of\n",
        "# measuring the amount of error in the prediction.\n",
        "mae = training_info.history['mae']\n",
        "validation_mae = training_info.history['val_mae']\n",
        "\n",
        "plt.plot(epochs[SKIP:], mae[SKIP:], 'g.', label='Training MAE')\n",
        "plt.plot(epochs[SKIP:], validation_mae[SKIP:], 'b.', label='Validation MAE')\n",
        "plt.title('Training and validation mean absolute error')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIuFqKIaSV4o"
      },
      "source": [
        "We can see that metrics are better for validation than training and that means the network is not overfitting. This means our network seems to be performing well! To confirm, let's check its predictions against the **Test** dataset we set aside earlier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0v9ZXwVSWxE"
      },
      "source": [
        "# Calculate and print the loss on our test dataset\n",
        "loss = model.evaluate(x_test, y_test)\n",
        "\n",
        "# Make predictions based on our test dataset\n",
        "predictions = model.predict(x_test)\n",
        "\n",
        "# Graph the predictions against the actual values\n",
        "plt.clf()\n",
        "plt.title('Comparison of predictions and actual values')\n",
        "plt.plot(x_test, y_test, 'b.', label='Actual')\n",
        "plt.plot(x_test, predictions, 'r.', label='Predicted')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5a6IjTdiWHT"
      },
      "source": [
        "Looks really great! The model isn't perfect; its predictions don't form a smooth sine curve. If we wanted to go further, we could try further increasing the capacity of the model.\n",
        "\n",
        "However, an important part of machine learning is knowing when to quit, and this model is good enough for our use case - which is to show a sine wave pattern on an LCD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWfgN3nRSmnM"
      },
      "source": [
        "## Generate a TensorFlow Lite Model\n",
        "\n",
        "We now have an acceptably accurate model. We'll use the [TensorFlow Lite Converter](https://www.tensorflow.org/lite/convert) to convert the model into a special, space-efficient format for use on memory-constrained devices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R377khIpjZO4"
      },
      "source": [
        "# Convert the model to the TensorFlow Lite format with quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "# Set the optimization flag.\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(\"sinewave_model.tflite\", \"wb\").write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJKfIV1ykJE1"
      },
      "source": [
        "The model size is 2.5KB! Sweet!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FP1ORwEkcCz"
      },
      "source": [
        "# Generate C files\n",
        "Let's generate C source and header file of this model for microcontroller. TF Lite has a Python method to convert TF Lite model into C source and header files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAjI_FS-nIEs"
      },
      "source": [
        "from tensorflow.lite.python.util import convert_bytes_to_c_source\n",
        "\n",
        "source_text, header_text = convert_bytes_to_c_source(tflite_model,\n",
        "                                                     \"sine_model\",\n",
        "                                                     include_path=\"sine_model.h\")\n",
        "with open('sine_model.h', 'w') as file:\n",
        "    file.write(header_text)\n",
        "\n",
        "with open('sine_model.cpp', 'w') as file:\n",
        "    file.write(source_text)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}